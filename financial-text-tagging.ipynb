{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1914f127-da97-4cf1-8134-a52d11d82601",
   "metadata": {},
   "source": [
    "Name: Seyed Morteza Hosseini\\\n",
    "This is my solution for Thomson Reuters data science challeng\\\n",
    "Here I answer to the questions of the challenge, then the code starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed6aa5e-3ed9-4747-81dc-d9e368d86494",
   "metadata": {},
   "source": [
    "1. There are multiple things that can be addressed for a solution to this problem\n",
    "   * Text Classification with Traditional ML Algorithms: We can start with traditional algorithms such as Naive Bayes, SVM, or Random Forests as a baseline. We can use TF-IDF to convert text to a vector format that the algorithms can process.\n",
    "   * Deep Learning with Word Embeddings: We can use word embeddings like Word2Vec, GloVe, or fastText with neural networks like RNNs to capture semantic meaning better than the traditional models. This approach could potentially lead to better performance if the dataset is large enough.\n",
    "   * Transfer Learning with Pre-trained LMs: Utilizing NLP models like BERT, GPT, or RoBERTa that have been pre-trained on large corpuses of text can be highly effective. These models can be fine-tuned on the specific dataset of customer queries to achieve high accuracy. This is a part of the sulotion that I used in this notebook.\n",
    "   * Ensemble Methods: Combining multiple models can often lead to better performance than any single model. An ensemble could average the predictions from a variety of models or use more sophisticated methods like stacking. For example we can use multiple types of models such as different flavour of BERTS and ensemble them together.\n",
    "   * Data Augmentation: To enhance the dataset size and variability, techniques like synonym replacement, back-translation, or generative models could be employed to create additional training data. I used synonym replacement to expand my train data, with a very simple method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35045f42-24b0-43d4-b81d-4f4b7206cd44",
   "metadata": {},
   "source": [
    "2. The implementation comes after the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff5a98-ccd0-4678-8bd6-ae70cf2de069",
   "metadata": {},
   "source": [
    "3. Metrics reported down bellow, I wanted to have multiple metrics for different changes that I made to the model, but due to the lack of time I only report the latest that I get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e8c73-b299-4561-9d4b-a8f7ccde6d96",
   "metadata": {},
   "source": [
    "4. The current performance is good ~90%, the business can use this as a even a user facing model, this can lower the call center queries. If the model performance was worse ~60% I would suggest that the business donnot put it infront of the user directly, and put a couple of humans in the loop to refine the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2a4b2-03ae-4d1c-9294-0e4fe80ae996",
   "metadata": {},
   "source": [
    "5. For this type of the problems (text), data is very important, when I populated train data with synonym generated data it enhanced the performance, so I would gather the data from as many sources as I can. Other types of data generation like with an LLM could be useful, but we have to watch out for model helucination and miss classifications. Also if I had time, I would definately try ensembling method, as they tend to increase the performance and decrease the variance and pervent overfitting in these type of problems. Next steps would be adding infrastructure for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533303e0-78a5-47dd-9183-6af8460e7d30",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ff66fc-16b5-4e49-9a6c-8993d4c7bddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 02:07:26.084878: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-08 02:07:26.122030: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-08 02:07:26.122054: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-08 02:07:26.123250: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-08 02:07:26.129738: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-08 02:07:26.807833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/morteza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/morteza/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding,  AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import datasets\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "TEST_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6e156-e361-405a-a0cd-12e165bec270",
   "metadata": {},
   "source": [
    "# Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909218b4-e307-4cd8-8655-b23e92fae03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            if word != l.name():\n",
    "                synonyms.append(l.name())\n",
    "    return synonyms\n",
    "\n",
    "def find_synonym(text):\n",
    "    sentences = [text]\n",
    "    texts = text.split()\n",
    "    texts = texts[:len(texts) // 2]\n",
    "    word_bag = [i for i in texts if i not in stopwords.words('english') and any(map(str.isupper, i)) is False]\n",
    "    word_bag = list(set(word_bag))\n",
    "    n = 2  #  each generated text will have two words replaced with their respective synonym \n",
    "    combined_bag = [word_bag[i * n:(i + 1) * n] for i in range((len(word_bag) + n - 1) // n)]\n",
    "    to_exchange = combined_bag\n",
    "    payload = text\n",
    "    for words in to_exchange:\n",
    "        for word in words:\n",
    "            similar_words = get_synonyms(word)\n",
    "            if similar_words is not None:\n",
    "                similar_words = [re.sub('[^A-Za-z ]+', ' ', sent) for sent in similar_words]\n",
    "                for similar in similar_words:\n",
    "                    payload = re.sub(word, similar, payload)\n",
    "                sentences.append(payload)\n",
    "    sentences = list(set(sentences))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea26380d-cd3b-4830-8d12-90a48a3fed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stop = lambda text: ' '.join([word for word in text.split() if word not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18183c3a-5304-4032-9b50-bc0b6a799cc1",
   "metadata": {},
   "source": [
    "## We can see the dataset is fairly balanced among labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e11b45f-4e15-4355-a3a5-3820c9233fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'label'}>]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_csv = pd.read_csv(\"./ds_task_dataset.csv\")\n",
    "dataset_csv.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed685643-2ade-4f39-b267-d341d8ec61da",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"CARD_PAYMENT_FEE_CHARGED\", \n",
    "    1: \"DIRECT_DEBIT_PAYMENT_NOT_RECOGNISED\",\n",
    "    2: \"BALANCE_NOT_UPDATED_AFTER_CHEQUE_OR_CASH_DEPOSIT\",\n",
    "    3: \"WRONG_AMOUNT_OF_CASH_RECEIVED\",\n",
    "    4: \"CASH_WITHDRAWAL_CHARGE\",\n",
    "    5: \"TRANSACTION_CHARGED_TWICE\",\n",
    "    6: \"DECLINED_CASH_WITHDRAWAL\",\n",
    "    7: \"TRANSFER_FEE_CHARGED\",\n",
    "    8: \"TRANSFER_NOT_RECEIVED_BY_RECIPIENT\",\n",
    "    9: \"BALANCE_NOT_UPDATED_AFTER_BANK_TRANSFER\",\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6adcbdc4-4a18-4b23-ad3b-bfa0243d6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bank_ds(split=TEST_SPLIT):\n",
    "    dataset = datasets.Dataset.from_pandas(dataset_csv).train_test_split(test_size=split)\n",
    "    ds = dataset[\"train\"].to_pandas()\n",
    "    ds[\"text\"] = ds[\"text\"].apply(find_synonym)\n",
    "    ds = ds.explode('text')\n",
    "    ds = ds.reset_index(drop=True)\n",
    "    dataset[\"train\"] = datasets.Dataset.from_pandas(ds)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c2c251d-0360-43b3-83b8-8d710688f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, num_labels=10, id2label=id2label, label2id=label2id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=10, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c500376-041e-44f5-9e30-a8139e64dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    examples[\"text\"] = list(map(lambda text: remove_stop(text), examples[\"text\"]))\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "def prepropcess_datasets(fn, *datasets):\n",
    "    return [ds.map(fn, batched=True) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1ad401-b1f1-4e73-9d51-011335fedf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compute_metrics():\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    recall = evaluate.load(\"recall\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    metrics = {\"accuracy\": (accuracy, {}), \"recall\": (recall, {\"average\": \"micro\"}), \"f1\": (f1, {\"average\": \"micro\"})}\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return {name: metric.compute(predictions=predictions, references=labels, **arg) for name, (metric, arg) in metrics.items()}\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a8fc8ab-e4cd-404c-bc84-5558c3e4c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_bank_ds()\n",
    "model, tokenizer = get_model(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "274be865-bbd0-4c41-87ba-92848c8d2583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb9cd2e003246eea44d9b828991342c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ae43590ecd4e928accb46de9b92ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = prepropcess_datasets(preprocess_function, dataset)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80ca9018-461d-4609-adb4-853deda49d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "compute_metrics = get_compute_metrics()\n",
    "\n",
    "def train_model(model, tokenizer, train_ds, test_ds):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"text-tagger\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e0133c-b688-41ce-a98a-dbeeb96ae5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt')\n",
    "    model = model.cpu()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    return model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa430144-981d-426d-ad03-1c5fcda6fb7a",
   "metadata": {},
   "source": [
    "### The result is pretty good, The data generation helped alot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a56f89c2-df65-46cd-bb8a-8f5ebd86e2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='785' max='785' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [785/785 01:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.236817</td>\n",
       "      <td>{'accuracy': 0.9370199692780338}</td>\n",
       "      <td>{'recall': 0.9370199692780338}</td>\n",
       "      <td>{'f1': 0.9370199692780338}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.252694</td>\n",
       "      <td>{'accuracy': 0.9354838709677419}</td>\n",
       "      <td>{'recall': 0.9354838709677419}</td>\n",
       "      <td>{'f1': 0.9354838709677419}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.241315</td>\n",
       "      <td>{'accuracy': 0.9431643625192012}</td>\n",
       "      <td>{'recall': 0.9431643625192012}</td>\n",
       "      <td>{'f1': 0.9431643625192012}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.250815</td>\n",
       "      <td>{'accuracy': 0.9431643625192012}</td>\n",
       "      <td>{'recall': 0.9431643625192012}</td>\n",
       "      <td>{'f1': 0.9431643625192012}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.244936</td>\n",
       "      <td>{'accuracy': 0.9493087557603687}</td>\n",
       "      <td>{'recall': 0.9493087557603687}</td>\n",
       "      <td>{'f1': 0.9493087557603687}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory text-tagger/checkpoint-157 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23681682348251343, 'eval_accuracy': {'accuracy': 0.9370199692780338}, 'eval_recall': {'recall': 0.9370199692780338}, 'eval_f1': {'f1': 0.9370199692780338}, 'eval_runtime': 0.493, 'eval_samples_per_second': 1320.563, 'eval_steps_per_second': 42.599, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "result = train_model(model, tokenizer, tokenized_ds[\"train\"], tokenized_ds[\"test\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf292c-f560-458d-b99a-6d7fcffa581b",
   "metadata": {},
   "source": [
    "### Lets try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14d88a16-12dc-4cac-bc9f-9cabbbe34764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT:  After transfering money balance changed. TRUE LABEL:  BALANCE_NOT_UPDATED_AFTER_BANK_TRANSFER PREDICTED LABEL:  BALANCE_NOT_UPDATED_AFTER_BANK_TRANSFER\n"
     ]
    }
   ],
   "source": [
    "print(\"TEXT: \", tokenized_ds[\"test\"][0]['text'], \"TRUE LABEL: \", id2label[tokenized_ds[\"test\"][0]['label']], \"PREDICTED LABEL: \", inference(model, tokenized_ds[\"test\"][0]['text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
